<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>This is my paper title</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="AI broadcasting." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:32px">AI broadcasting:Multi-modal Learning based AI Broadcast Production</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Xi Li</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Qiwen Gan</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Zhicheng Wang</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://www.mathimaging.org/">Jiangtao Wen</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/w16311/webpage-template'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:1000px" src="./resources/sampleteaser.png"/>
					</center>
				</td>
			</tr>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				<p style="text-align:justify; text-justify:inter-ideograph;">
				Live broadcast production using multiple cameras normally requires a large team with a multitude of technical skills, artistic expertise, as well as domain knowledge about the event to be produced, e.g. symphonies, ballets, and various sport events. The current approach to directing and producing live performances is labor-intensive, time-consuming, and lacks support for post-production editing and personalizations. 
The paper proposes a solution to these issues in the form of an AI-based automatic broadcast production system. This system utilizes multiple ultra-high-definition (UHD) cameras focused  strategically on the stage capturing ``static shots" without camera movements and human interventions. From these static footage, the system performs feature extraction and analysis on the captured video stream and audio, and uses digital video and audio processing techniques to simulate camera movements such as panning, zooming-in/zooming out, and depths-of-field. The digital effects are controlled by an AI system that analyzes video and audio streams in real time, as well as side information such as sports rule books, music scores and play scripts. Higher level information such as live performer-audience interaction, facial expressions of key ``players'' in the scene, salience of audio and video, viewing habits and interests of the target audience etc. can also be incorporated into the AI decision loop, to produce one or many high definition streams that emulates the output of a highly-skilled human production team, dedicated to a specific target audience. The proposed system has the potential to post-process infinitely, or for any number of target demographic groups from the same raw UHD captures. The system can drastically lower the production cost of broadcasting live events, providing 
 targeted and optimized content to a large variety of demographics without incurring significant additional cost. As an initial demonstration of the feasibility of a real time, low latency, high definition quality AI based automatic production system, the system described in the paper can be improved in many areas in terms of quality, speed and latency. It can be further enhanced by incorporating high degress of freedom (DoF) and free-viewpoint video synthesis,immersive audio and virtual reality presentations. 
				</p>
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Demo</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/w7tjpLa3Nig" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>
	<hr>

	<center><h1>Overview</h1></center>
	<table align=center width=850px>
		<tr>
			<td>
				<p style="text-align:justify;">Our system is composed of four components: the ST (Switching Timeline) system, the ROI (Region of Interest) system, the Camera Selector system, and post-processing.</p>
				<br>
				<center>
						<img class="round" style="width:800px" src="./resources/pipline.png"/>
				</center>
				<br>
				<p style="text-align:justify;">
					<strong>ST system:</strong> The 2D audio spectrogram is divided into a sequence of 16x16 overlapping patches, which are then
					linearly transformed into 1-D patch embeddings. These embeddings are augmented with a learnable positional embedding and
					a classification token is added to the start of the sequence. The resulting embedding is fed into a Transformer and the output of
					the classification token is used for classification of 527 classes for sound event localization.
				</p>
				<br>
				<center>
					<img class="round" style="width:800px" src="./resources/st.png"/>
				</center>
				<br>
				<p style="text-align:justify;">
					<strong>ROI system:</strong> It contains a Sound-source
					Localization module and Object detection module. The source
					localization module uses separate audio and video image networks
					for audio and visual feature extraction. 
				</p>
				<br>
				<center>
					<img class="round" style="width:800px" src="./resources/ROI.png"/>
				</center>
				<br>
				<p style="text-align:justify;">
					<strong>Camera Selector system:</strong> We incorporated a Highlight Ranking Net to score the
					ROI images. 
				</p>
				<br>
				<p style="text-align:justify;">
					<strong>Post-processing:</strong> To avoid excessive shot jitters, we first smooth the bounding
					boxes temporally for the chosen camera. For the chosen camera however, the bound-
					ing box may still jitter due to various factors.
				</p>
				<br>
				<center>
					<img class="round" style="width:800px" src="./resources/CameraSelector.png"/>
				</center>
				<br>
			</td>
		</tr>
	</table>
	<hr>

	<center><h1>Code</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					The code will be released soon!
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/richzhang/webpage-template'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>
	<hr>
	<!-- <table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">F. Author, S. Author, T. Author.<br>
				<b>Creative and Descriptive Paper Title.</b><br>
				In Conference, 20XX.<br>
				(hosted on <a href="">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<!-- <table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table> -->

	<!-- <hr>
	<br> --> -->

	<!-- <table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table> -->

<!-- <br> -->
</body>
</html>

